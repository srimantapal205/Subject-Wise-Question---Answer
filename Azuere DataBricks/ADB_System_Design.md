# Azure DataBricks System Design
---

## ğŸš€ **System Design Interview: Handling 500GB Daily with PySpark**

## ğŸ¯ **Scenario**

You are tasked with designing and optimizing a **PySpark-based data pipeline** that processes **500 GB of data per day**.
Your goal: build a **scalable, cost-efficient, and high-performance architecture** â€” and explain **how to size the cluster**.

---

## ğŸ§± **1. High-Level Architecture**

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Data Sources       â”‚
                â”‚ (CSV / JSON / APIs) â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Raw Zone (ADLS / S3)        â”‚
                â”‚  - Landing 500 GB daily      â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Processing Layer (PySpark)   â”‚
                â”‚  - Cleansing / Joins / ETL    â”‚
                â”‚  - Incremental Processing     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Storage Layer (Delta Lake)  â”‚
                â”‚  - Optimized for Analytics    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Serving Layer (Power BI, SQL) â”‚
                â”‚  - Dashboards & Reports       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ **2. The 5-Step Optimization Blueprint**

### **Step 1ï¸âƒ£ â€“ Format Optimization**

**Action:** Convert raw data (CSV/JSON) â†’ **Parquet or Delta Lake** format immediately.
**Why:**

* Columnar compression
* Predicate pushdown
* Reduced I/O

âœ… **Boosts performance by 3â€“5Ã—**

---

### **Step 2ï¸âƒ£ â€“ Partitioning Strategy**

Each Spark task â‰ˆ **128 MB** of data.

**Calculation:**

```
500 GB Ã— 1024 MB Ã· 128 MB = 4000 partitions
```

â¡ï¸ Spark now executes **~4000 parallel tasks**, ensuring balanced processing.

ğŸ“Š **Partitioning Recommendation:**

* Partition by `ingestion_date`, `region`, or `business_key` for incremental loading.

---

### **Step 3ï¸âƒ£ â€“ Cluster Sizing**

**Assumptions:**

* 10 worker nodes
* 8 cores & 32 GB RAM per node

**Parallelism Calculation:**

```
10 nodes Ã— 8 cores = 80 cores
Each core handles 2â€“3 tasks  â†’ ~240 tasks concurrently
```

**Execution Time Estimate:**

```
4000 Ã· 240 â‰ˆ 17 waves of execution
1â€“2 minutes per wave â†’ ~25â€“30 minutes total runtime
```

âœ… **Balanced trade-off** between cost and performance.

---

### **Step 4ï¸âƒ£ â€“ Memory Management**

Spark needs ~**3Ã— data volume** during joins and shuffles.

**Memory Requirement:**

```
(500 GB Ã— 3) Ã· 10 nodes = 150 GB per node
```

With **32 GB per node**, some disk spill is expected (acceptable with SSD).
For heavy joins â†’ use **64 GB nodes**.

ğŸ’¡ **Best Practices:**

* Use broadcast joins for small lookup tables
* Persist only reusable dataframes
* Optimize shuffle partitions

---

### **Step 5ï¸âƒ£ â€“ Performance Tuning**

| Setting                             | Recommended Value | Purpose                             |
| ----------------------------------- | ----------------- | ----------------------------------- |
| `spark.sql.shuffle.partitions`      | `400`             | Reduces shuffle overhead            |
| `spark.sql.adaptive.enabled`        | `true`            | Enables adaptive query optimization |
| `spark.sql.files.maxPartitionBytes` | `128MB`           | Controls input split size           |

âœ… **Additional Tips:**

* Implement **Incremental Loads** using Delta MERGE
* Avoid full reloads
* Compact small files (`OPTIMIZE` + Z-ORDER in Databricks)

---

## ğŸ’¾ **3. Cost vs. Performance Balance**

| Parameter       | Trade-Off          | Example Decision                         |
| --------------- | ------------------ | ---------------------------------------- |
| Cluster Size    | Cost â†‘, Speed â†‘    | Choose 10 nodes, scale up only if needed |
| File Format     | Speed â†‘, Storage â†“ | Use Delta/Parquet over CSV               |
| Partition Count | More parallelism   | Target 128 MB per partition              |
| Memory          | Avoid spill        | Consider SSDs or larger memory nodes     |

---

## ğŸ§  **4. Key Takeaways**

* Optimize **format, partitioning, and configuration** before scaling up compute.
* Measure and adjust **task size (128 MB)** and **shuffle partitions (400)**.
* Implement **incremental and adaptive** processing strategies.

---

## ğŸ§­ **5. Final Interview Summary**

> â€œTo process 500 GB daily in PySpark efficiently,
> Iâ€™d architect a **Delta Lake-based pipeline** with **optimized partitions, adaptive execution, and right-sized clusters.**
> Performance is not just about adding nodes â€” itâ€™s about designing smart, efficient systems.â€

---

Would you like me to include a **Databricks-specific version** of this (showing cluster configuration + adaptive execution diagram) so you can use it as a visual reference in interviews or presentations?
