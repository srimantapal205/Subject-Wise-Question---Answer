# ğŸ“‹ ADF Scenario-Based Questions

### ğŸ”· Data Ingestion Scenarios

1. **How would you design a pipeline to ingest incremental data from an on-prem SQL Server to Azure Data Lake every hour?**

   * Follow-up: How will you handle late-arriving data?



âœ… Great question â€” this is a **common real-world scenario** and a typical interview topic for Data Engineers working with Azure.

Letâ€™s walk through both parts step by step:

---

# ğŸš€ **Pipeline to ingest incremental data from on-prem SQL Server to Azure Data Lake every hour**

### ğŸ”· **Requirements**

* Source: On-premises **SQL Server**
* Destination: **Azure Data Lake (ADLS Gen2)**
* Frequency: **Hourly**
* Only **incremental data** (changes since last load)
* Must also handle **late-arriving data**

---

# ğŸ¯ **Design Steps**

---

## 1ï¸âƒ£ **Connectivity**

* Use **Self-hosted Integration Runtime (SHIR)** to connect securely to your on-prem SQL Server from Azure.

  * Install SHIR on a VM or server in your network.
  * Register it with Azure Data Factory (ADF).

---

## 2ï¸âƒ£ **Detect Incremental Data**

There are several ways to identify incremental changes:

* If the source table has a column like:

  * `LastModifiedDateTime` (preferred)
  * `CreatedDateTime`
  * or a **change tracking / CDC mechanism**

### Options:

| Method                        | Notes                                                                                                                                               |
| ----------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Timestamp column**          | If a column like `LastModifiedDateTime` exists, store the **max timestamp** loaded in each run. Query: `WHERE LastModifiedDateTime > @LastMaxTime`. |
| **Change Tracking (CT)**      | SQL Server feature â€” lightweight, tracks changes since a version. Use the **version token** for next run.                                           |
| **Change Data Capture (CDC)** | SQL Server feature â€” more detailed than CT. Reads from CDC tables to get inserts/updates/deletes.                                                   |

For most scenarios:
âœ… simplest: `LastModifiedDateTime`
âœ… better for updates & deletes: CT or CDC.

---

## 3ï¸âƒ£ **Build the ADF Pipeline**

### Components:

* **Lookup** activity â†’ fetch last watermark from a control table / blob file.
* **Source Dataset** â†’ SQL Server table, with query:

  ```sql
  SELECT * FROM Table WHERE LastModifiedDateTime > @last_watermark
  ```
* **Copy Activity** â†’ write to ADLS in Parquet/CSV with folder structure:

  ```
  /raw_data/TableName/YYYY/MM/DD/HH/
  ```
* After successful load â†’ update the watermark for next run.

---

## 4ï¸âƒ£ **Schedule**

* Use **ADF Trigger** (time-based) â†’ run **every hour on the hour**.

---

## 5ï¸âƒ£ **Schema & Partitioning**

* Store data in ADLS partitioned by date/hour:

  ```
  adls://datalake/container/table_name/year=2025/month=07/day=05/hour=18/
  ```
* Choose **Parquet** for efficient storage & analytics.

---

---

# ğŸ“¦ **Handling Late-Arriving Data**

Late-arriving data means records that belong to an earlier period (e.g., a transaction for 10 AM arrives at 2 PM).

### Strategies:

---

### ğŸŒŸ 1. **Do Nothing (if acceptable)**

If your downstream processing tolerates late data, and consumers always query the **latest snapshot**, you may not need to correct history.

---

### ğŸŒŸ 2. **Reprocess Window**

* Keep a **rolling window of N hours/days** in your ingestion pipeline.
* Every hour:

  * Load data modified in the **last 2â€“3 hours** (instead of just 1).
  * This catches late updates.
  * Example query:

    ```sql
    WHERE LastModifiedDateTime > @last_watermark - 2 hours
    ```
* In ADLS:

  * Overwrite the partitions corresponding to those hours/days to ensure accuracy.

---

### ğŸŒŸ 3. **Delta Lake / Upserts**

* Store data in **Delta Lake** format on ADLS.
* With Delta tables, you can **merge/upsert** late-arriving records into existing partitions.
* This is more sophisticated & ensures correctness over time.

---

### ğŸŒŸ 4. **Audit & Alerts**

* Optionally track how much late data is arriving.
* Trigger alerts if lateness > threshold â†’ so the source system can be improved.

---

# âœ¨ **Summary Table**

| Feature            | Implementation                                        |
| ------------------ | ----------------------------------------------------- |
| Incremental Data   | Use `LastModifiedDateTime`, CT, or CDC                |
| Pipeline           | ADF pipeline â†’ SHIR â†’ SQL â†’ ADLS                      |
| Schedule           | Hourly trigger                                        |
| Watermark          | Store in SQL table / Blob metadata                    |
| Data Format        | Parquet or Delta, partitioned by date/hour            |
| Late Data Handling | Reprocess rolling window OR use Delta Lake with MERGE |

---

# ğŸ“‹ Example Architecture:

```
On-prem SQL Server
    â¬‡ï¸ (via SHIR)
ADF Pipeline
    â¬‡ï¸ (Copy Activity)
ADLS Gen2 â†’ partitioned folders or Delta table
    â¬‡ï¸
(Downstream processing: Databricks, Synapse, Power BI)
```

---




2. **You need to copy files from an SFTP server to Azure Blob Storage and send an email to the manager if any file is missing. How would you implement this?**
âœ… Good scenario â€” let me walk you through a **design** for this use case step by step:

> ğŸ“„ **Goal**: Copy files from SFTP â†’ Azure Blob Storage, and if any expected file(s) are missing, send an email to the manager.

---

### ğŸ”· **High-level steps**

1ï¸âƒ£ **Define the list of expected files**

* This could come from:

  * A static list configured in your pipeline (e.g., parameter or config file).
  * A dynamic list fetched from a database or metadata store.

---

2ï¸âƒ£ **Pipeline implementation:**

Weâ€™ll use **Azure Data Factory (ADF)** or **Synapse Pipelines** since they support SFTP, Blob Storage, and email notification.

---

### ğŸ“ **ADF Pipeline Design**

#### ğŸ“‚ 1. Get the list of actual files from SFTP

* Use a **Get Metadata activity** to list files in the SFTP folder.

  * Configure a Linked Service to connect to the SFTP server.
  * Use a Dataset pointing to the folder on SFTP.
  * The output will be an array of file names.

---

#### ğŸ“œ 2. Compare actual vs expected

* Pass the list of expected files as a pipeline parameter or retrieve it from a Lookup activity.
* Use a **Set Variable** or a **Stored Procedure** / **Mapping Data Flow** / custom **Azure Function** / **Until activity** with an **If Condition** to compare the expected list and the actual list.

  * Identify which files are missing.

---

#### ğŸ“¤ 3. Copy files

* For each file in the actual list:

  * Use a **ForEach activity** to loop over the files.
  * Inside the ForEach, use a **Copy activity**:

    * Source: SFTP file.
    * Sink: Azure Blob Storage container.

---

#### ğŸ“§ 4. Notify manager if files are missing

* If any expected file is not present on the SFTP, trigger a **Web activity** to call:

  * Logic App
  * Power Automate
  * Or directly use the ADF **Send Email (Preview)** activity (if available in your region)
  * The email body can include which files were missing.

---

### ğŸ“„ Example flow in ADF:

```
[Lookup: Expected file list]
          |
     [Get Metadata: SFTP folder file list]
          |
  [Compare lists: identify missing files]
          |
        /   \
[If no missing] [If missing]
   |               |
[ForEach: Copy]   [Send Email]
```

---

### ğŸ”· Notes:

âœ… Make sure the SFTP Linked Service has proper credentials (key or password).
âœ… Use a dynamic dataset in the Copy activity to handle different file names.
âœ… For comparison logic, you can use:

* Data Flow (with Left Anti Join between expected & actual).
* Stored Procedure in SQL.
* Custom Azure Function for complex logic.
  âœ… For the email, if ADF native email isnâ€™t available, a Logic App or Function is the usual choice.

---

### Sample tools:

| Step          | Tool/Activity                |
| ------------- | ---------------------------- |
| Expected list | Lookup or Pipeline parameter |
| Actual list   | Get Metadata                 |
| Compare lists | Data Flow or pipeline logic  |
| Copy files    | Copy activity in ForEach     |
| Send email    | Web activity â†’ Logic App     |

---

### ğŸ’¡ Example Email:

```
Subject: Missing files detected on SFTP

Hello Manager,

The following expected files were not found on the SFTP server during todayâ€™s run:
- file1.csv
- file2.csv

Please investigate.

Thanks,
Data Engineering Team
```

---


3. **Youâ€™re asked to ingest data from 50 different REST APIs with different authentication mechanisms. How would you make the pipeline reusable and dynamic?**

âœ… This is a **classic scenario** in data engineering where you need to ingest data from many REST APIs â€” each with **different authentication mechanisms** â€” and you want to build a **reusable & dynamic pipeline** rather than 50 hard-coded pipelines.

Hereâ€™s how you can design and implement it, step by step:

---

## ğŸ¯ Goals:

* Handle multiple APIs (50+).
* Support different authentication types (API key, OAuth2, Basic auth, etc.).
* Make the pipeline reusable â€” no code changes when adding a new API.
* Make it dynamic â€” parameters driven by configuration.

---

## ğŸš€ Solution Design

### 1ï¸âƒ£ **Use a Metadata/Configuration-driven Approach**

Create a **configuration store** (in a database table, JSON, or YAML file) that contains all the details for each API.

Example schema for the config table:

| api\_name   | endpoint\_url                 | auth\_type  | auth\_details          | params | headers | schedule |
| ----------- | ----------------------------- | ----------- | ---------------------- | ------ | ------- | -------- |
| WeatherAPI  | `https://api.weather.com/...` | API\_KEY    | `{ "key": "abc123" }`  | `{â€¦}`  | `{â€¦}`   | daily    |
| TwitterAPI  | `https://api.twitter.com/...` | OAUTH2      | `{ "client_id": â€¦ }`   | `{â€¦}`  | `{â€¦}`   | hourly   |
| InternalAPI | `https://myapp.local/...`     | BASIC\_AUTH | `{ "user":â€¦, "pwd":â€¦}` | `{â€¦}`  | `{â€¦}`   | weekly   |

---

### 2ï¸âƒ£ **Design the Pipeline**

You can build the pipeline in **Azure Data Factory**, **Databricks**, or any orchestration tool (Airflow, Prefect).
The key is:

* Read the configuration.
* Loop over each API config.
* Call the REST API with appropriate auth and params.
* Store the response.

---

### 3ï¸âƒ£ **Implement Authentication Handlers**

Write reusable functions or modules to handle different `auth_type`:

* If `auth_type = API_KEY`: Inject the key into header or query string.
* If `auth_type = BASIC_AUTH`: Base64 encode user\:password.
* If `auth_type = OAUTH2`: Obtain token, refresh if expired, and attach to header.
* If `auth_type = NONE`: Just call.

You can implement these as Python functions, or ADF Web Activity dynamic expressions.

---

### 4ï¸âƒ£ **Dynamic Pipeline in ADF (example)**

In **ADF**:

* Use a **Lookup activity** to read the configuration (from SQL table, Blob JSON, etc.)
* Use a **ForEach activity** to iterate over each API.
* Inside ForEach:

  * Use a **Web activity** (or Azure Function) to call the API.
  * Pass dynamic `url`, `headers`, `body`, etc. from the config.
  * Handle the response and save it (Blob, ADLS, SQL, etc.)
  * Optionally log the status.

---

### 5ï¸âƒ£ **Dynamic Pipeline in Databricks (example)**

In **Databricks (PySpark)**:

* Read the config table into a DataFrame.
* Loop through rows:

  ```python
  import requests
  import base64

  for row in config_df.collect():
      url = row['endpoint_url']
      auth_type = row['auth_type']
      auth_details = json.loads(row['auth_details'])
      headers = json.loads(row['headers']) or {}
      params = json.loads(row['params']) or {}

      if auth_type == 'API_KEY':
          headers['Authorization'] = f"Bearer {auth_details['key']}"
      elif auth_type == 'BASIC_AUTH':
          token = base64.b64encode(f"{auth_details['user']}:{auth_details['pwd']}".encode()).decode()
          headers['Authorization'] = f"Basic {token}"
      elif auth_type == 'OAUTH2':
          # implement token retrieval & attach
          pass
      
      response = requests.get(url, headers=headers, params=params)
      # save response to storage
  ```

---

### 6ï¸âƒ£ **Logging & Monitoring**

* Log success/failure for each API call.
* Capture response status & any error messages.
* You can store logs in a table or monitoring tool.

---

### 7ï¸âƒ£ **Adding New APIs**

Once this framework is in place:

* To add a new API, just add a new record to the config table/file.
* No code or pipeline change required.

---

## ğŸ“ Benefits

âœ… Scalable: Easily supports >50 APIs.
âœ… Reusable: Single pipeline for all APIs.
âœ… Configurable: Changes driven by metadata.
âœ… Auditable: Logs everything centrally.

---

## Example Tech Stack:

| Task          | Tools                              |
| ------------- | ---------------------------------- |
| Orchestration | ADF, Airflow, Databricks workflows |
| Config Store  | Azure SQL DB, Cosmos DB, Blob JSON |
| API Calls     | ADF Web Activity / Python requests |
| Storage       | ADLS Gen2, Blob, SQL               |
| Monitoring    | Azure Monitor, Log Analytics       |

---


4. **If your source table doesnâ€™t have a column that indicates updated/inserted records, how can you implement incremental load?**

âœ… Great question â€” and a very common challenge in data engineering & ETL design!

If your **source table doesnâ€™t have a column to indicate which records were inserted/updated (like a timestamp, last\_modified, or change\_flag)**, then you **cannot do a simple incremental load based on a watermark.**

But you *can still implement incremental loading* using one of the following strategies:

---

## ğŸš€ Options to Implement Incremental Load Without a Change Column

---

### 1ï¸âƒ£ **Full Extract + Change Data Capture (CDC) Logic**

* Load the **entire source table** into a staging area (usually in your data lake or staging database).
* Compare the staging data with the target table to identify:

  * **New records** â†’ Not present in the target.
  * **Updated records** â†’ Present in the target but with different values.
  * **Unchanged records** â†’ Already in the target & identical.

You typically implement this using:

* A **hash or checksum** of each row to detect changes efficiently.
* Or by comparing all relevant columns directly.

**Steps:**

```sql
-- Step 1: Compute a hash of each record in source & target
SELECT id, md5(concat(col1, col2, col3, â€¦)) AS row_hash FROM source_table;
SELECT id, md5(concat(col1, col2, col3, â€¦)) AS row_hash FROM target_table;

-- Step 2: Join on `id` and compare hashes
-- If `id` not in target â†’ insert
-- If `id` in target & hash != â†’ update
-- If `id` in target & hash = â†’ skip
```

---

### 2ï¸âƒ£ **Change Data Capture at the Database Level**

* Some databases (SQL Server, Oracle, MySQL, PostgreSQL) support CDC features or write-ahead logs (WAL) you can tap into.
* Tools like:

  * **SQL Server CDC/CT (Change Tracking)**
  * **Debezium**
  * **GoldenGate**
  * **Azure Data Factory with CDC in Mapping Data Flows**
    can capture changes at the transaction/log level â€” even if your table itself doesnâ€™t store `last_updated`.

---

### 3ï¸âƒ£ **Triggers in Source Database**

If allowed, you can create a trigger on the source table to populate an auxiliary log table whenever rows are inserted/updated/deleted.
Your pipeline can then read from this log table to get the deltas.

---

### 4ï¸âƒ£ **Snapshots + SCD (Slowly Changing Dimension) Type 2**

* Take regular **snapshots** of the table (daily, hourly, etc.).
* Compare todayâ€™s snapshot to the previous snapshot.
* Insert changes or maintain history (like SCD Type 2).

---

### 5ï¸âƒ£ **External Tools**

If youâ€™re using cloud tools like:

* **Azure Data Factory**,
* **Databricks**,
* **Fivetran**,
* **Snowflake streams**,
  you can often configure them to detect & load changes by comparing snapshots or using built-in CDC features.

---

## ğŸ“Œ Key Techniques Used

| Technique                      | Pros                        | Cons                         |
| ------------------------------ | --------------------------- | ---------------------------- |
| Full extract + hash comparison | Simple, no DB changes       | High data transfer & compute |
| CDC / Log-based                | Efficient, minimal overhead | Requires DB support & setup  |
| Triggers                       | Works on any DB             | Adds write overhead          |
| Snapshots + diff               | No DB change, keeps history | Storage intensive            |

---

## ğŸ‘· Best Practice

âœ… If the source DB supports CDC â†’ use it.
âœ… If not, but data size is small â†’ use full extract + hash comparison.
âœ… If large & no CDC â†’ consider designing a lightweight trigger-based log.
âœ… Always try to push the responsibility for tracking changes to the **source system** if possible (add a `last_modified` column if feasible).

---



5. **Your pipeline needs to fetch yesterdayâ€™s sales data automatically. How would you parameterize and set up the pipeline for that?**


âœ… This is a very common **date-driven pipeline design question**, and a good answer shows you know how to make it **dynamic and reusable**.

Here is a clear explanation with **steps and an example** (using Azure Data Factory or Databricks, but the principles apply to any ETL tool):

---

### ğŸ¯ Goal:

Fetch **yesterdayâ€™s sales data** automatically every day â€” for example, process sales where `sales_date = yesterday`.

---

## ğŸš€ Approach

### 1ï¸âƒ£ **Parameterize the date**

* Define a **pipeline parameter** called `ProcessingDate`.
* You donâ€™t hard-code it; instead, calculate it dynamically.

In Azure Data Factory (ADF), you can use the expression:

```sql
@formatDateTime(addDays(utcnow(), -1), 'yyyy-MM-dd')
```

ğŸ“Œ This computes yesterdayâ€™s date at runtime.

If youâ€™re in Databricks:

```python
from datetime import datetime, timedelta
processing_date = (datetime.today() - timedelta(days=1)).strftime("%Y-%m-%d")
```

---

### 2ï¸âƒ£ **Use this parameter in your source query/path**

* If youâ€™re querying a database:

```sql
SELECT * 
FROM sales 
WHERE sales_date = '@{pipeline().parameters.ProcessingDate}'
```

* If youâ€™re reading from files (like `sales_YYYY-MM-DD.csv`):
  you can build the path dynamically:

```
/data/sales/sales_@{pipeline().parameters.ProcessingDate}.csv
```

---

### 3ï¸âƒ£ **Schedule the pipeline**

* Use a trigger to **run daily at a set time**, e.g., 2 AM.
* No manual intervention is needed since the date is calculated dynamically.

---

## ğŸ”· Optional Enhancements

âœ… Add a fallback: If the parameter is supplied manually, use it; else default to yesterday.
ADF example:

```sql
@if(equals(pipeline().parameters.ProcessingDate, ''), 
    formatDateTime(addDays(utcnow(), -1), 'yyyy-MM-dd'), 
    pipeline().parameters.ProcessingDate)
```

âœ… Add validation to ensure data for the date exists before processing.

---

### ğŸ“‹ Example Summary Table

| Step                           | What you do                                            |
| ------------------------------ | ------------------------------------------------------ |
| Define parameter               | `ProcessingDate`                                       |
| Default value (ADF expression) | `@formatDateTime(addDays(utcnow(), -1), 'yyyy-MM-dd')` |
| Use parameter in query/path    | `WHERE sales_date = '@ProcessingDate'` or file path    |
| Schedule pipeline              | Daily trigger (e.g., at 2 AM)                          |


---

### ğŸ”· Data Transformation & Data Flow Scenarios

6. **You have two datasets (customer & orders) in Blob Storage that need to be joined on `CustomerID`, aggregated, and written into Azure SQL Database. How would you build this?**

7. **How would you implement Slowly Changing Dimension (SCD) Type 2 logic in ADF Data Flows?**

8. **If the source file contains invalid/malformed rows, how would you handle and log them?**

9. **Your pipeline needs to convert files from CSV to Parquet and partition them by date before storing in ADLS. How would you achieve this?**

---

### ğŸ”· Orchestration & Monitoring Scenarios

10. **How would you design a pipeline that runs every day at 2AM, but only if the upstream system has deposited a new file in Blob Storage?**

11. **How would you retry a failed activity only 3 times and send a Teams notification if it still fails?**

12. **You need to execute pipelines in a specific order â€” Pipeline A â†’ Pipeline B â†’ Pipeline C, but only if A & B are successful. How would you design this?**

13. **How would you implement a mechanism to log every pipeline run status, start & end time, and activity outcomes into a SQL table for auditing?**

---

### ğŸ”· Performance & Optimization Scenarios

14. **What techniques would you use to optimize the performance of a copy activity transferring millions of rows?**

15. **How would you design your pipeline to scale when processing 1TB of daily data without failing?**

---

### ğŸ”· Dynamic & Parameterized Pipelines

16. **You have to load data from 100 tables of a database into their respective folders in ADLS. How would you build a single dynamic pipeline to handle this?**

17. **Your pipeline needs to read the schema of incoming files dynamically and load them to a staging table. How would you handle schema drift?**

---

### ğŸ”· Security & Access Scenarios

18. **If you need to securely connect to an on-prem SQL Server via Self-Hosted Integration Runtime, what steps would you take?**

19. **How would you ensure that sensitive information (like passwords or API keys) are not exposed in your pipelines?**

---

### ğŸ”· Advanced/Real-Time Scenarios

20. **Youâ€™re asked to implement a pipeline that triggers on arrival of a file, transforms it, and updates Power BI datasets in near real-time. How would you design it?**

21. **You have to process and validate thousands of small files arriving every hour and merge them into a single Parquet file. How would you build this?**

---

## ğŸ“Œ Bonus â€” Troubleshooting & Best Practices

22. **A pipeline that was running fine yesterday is now failing with a timeout error when writing to SQL Database. How would you debug it?**

23. **What are some best practices you follow for naming conventions, folder structure, and reusability in ADF?**

24. **How would you test your pipelines before moving them to production?**

25. **What would you do if the dataset schema at the source changed suddenly and broke your pipeline?**

---
