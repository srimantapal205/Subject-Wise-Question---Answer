# üìã ADF Scenario-Based Questions

### 1. **How would you design a pipeline to ingest incremental data from an on-prem SQL Server to Azure Data Lake every hour?**

   * Follow-up: How will you handle late-arriving data?



‚úÖ Great question ‚Äî this is a **common real-world scenario** and a typical interview topic for Data Engineers working with Azure.

Let‚Äôs walk through both parts step by step:

---

# üöÄ **Pipeline to ingest incremental data from on-prem SQL Server to Azure Data Lake every hour**

#### üî∑ **Requirements**

* Source: On-premises **SQL Server**
* Destination: **Azure Data Lake (ADLS Gen2)**
* Frequency: **Hourly**
* Only **incremental data** (changes since last load)
* Must also handle **late-arriving data**

---

# üéØ **Design Steps**

---

#### 1Ô∏è‚É£ **Connectivity**

* Use **Self-hosted Integration Runtime (SHIR)** to connect securely to your on-prem SQL Server from Azure.

  * Install SHIR on a VM or server in your network.
  * Register it with Azure Data Factory (ADF).

---

#### 2Ô∏è‚É£ **Detect Incremental Data**

There are several ways to identify incremental changes:

* If the source table has a column like:

  * `LastModifiedDateTime` (preferred)
  * `CreatedDateTime`
  * or a **change tracking / CDC mechanism**

#### Options:

| Method                        | Notes                                                                                                                                               |
| ----------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Timestamp column**          | If a column like `LastModifiedDateTime` exists, store the **max timestamp** loaded in each run. Query: `WHERE LastModifiedDateTime > @LastMaxTime`. |
| **Change Tracking (CT)**      | SQL Server feature ‚Äî lightweight, tracks changes since a version. Use the **version token** for next run.                                           |
| **Change Data Capture (CDC)** | SQL Server feature ‚Äî more detailed than CT. Reads from CDC tables to get inserts/updates/deletes.                                                   |

For most scenarios:
‚úÖ simplest: `LastModifiedDateTime`
‚úÖ better for updates & deletes: CT or CDC.

---

#### 3Ô∏è‚É£ **Build the ADF Pipeline**

##### Components:

* **Lookup** activity ‚Üí fetch last watermark from a control table / blob file.
* **Source Dataset** ‚Üí SQL Server table, with query:

  ```sql
  SELECT * FROM Table WHERE LastModifiedDateTime > @last_watermark
  ```
* **Copy Activity** ‚Üí write to ADLS in Parquet/CSV with folder structure:

  ```
  /raw_data/TableName/YYYY/MM/DD/HH/
  ```
* After successful load ‚Üí update the watermark for next run.

---

#### 4Ô∏è‚É£ **Schedule**

* Use **ADF Trigger** (time-based) ‚Üí run **every hour on the hour**.

---

#### 5Ô∏è‚É£ **Schema & Partitioning**

* Store data in ADLS partitioned by date/hour:

  ```
  adls://datalake/container/table_name/year=2025/month=07/day=05/hour=18/
  ```
* Choose **Parquet** for efficient storage & analytics.

---

---

## üì¶ **Handling Late-Arriving Data**

Late-arriving data means records that belong to an earlier period (e.g., a transaction for 10 AM arrives at 2 PM).

### Strategies:

---

### üåü 1. **Do Nothing (if acceptable)**

If your downstream processing tolerates late data, and consumers always query the **latest snapshot**, you may not need to correct history.

---

### üåü 2. **Reprocess Window**

* Keep a **rolling window of N hours/days** in your ingestion pipeline.
* Every hour:

  * Load data modified in the **last 2‚Äì3 hours** (instead of just 1).
  * This catches late updates.
  * Example query:

    ```sql
    WHERE LastModifiedDateTime > @last_watermark - 2 hours
    ```
* In ADLS:

  * Overwrite the partitions corresponding to those hours/days to ensure accuracy.

---

### üåü 3. **Delta Lake / Upserts**

* Store data in **Delta Lake** format on ADLS.
* With Delta tables, you can **merge/upsert** late-arriving records into existing partitions.
* This is more sophisticated & ensures correctness over time.

---

### üåü 4. **Audit & Alerts**

* Optionally track how much late data is arriving.
* Trigger alerts if lateness > threshold ‚Üí so the source system can be improved.

---

# ‚ú® **Summary Table**

| Feature            | Implementation                                        |
| ------------------ | ----------------------------------------------------- |
| Incremental Data   | Use `LastModifiedDateTime`, CT, or CDC                |
| Pipeline           | ADF pipeline ‚Üí SHIR ‚Üí SQL ‚Üí ADLS                      |
| Schedule           | Hourly trigger                                        |
| Watermark          | Store in SQL table / Blob metadata                    |
| Data Format        | Parquet or Delta, partitioned by date/hour            |
| Late Data Handling | Reprocess rolling window OR use Delta Lake with MERGE |

---

# üìã Example Architecture:

```
On-prem SQL Server
    ‚¨áÔ∏è (via SHIR)
ADF Pipeline
    ‚¨áÔ∏è (Copy Activity)
ADLS Gen2 ‚Üí partitioned folders or Delta table
    ‚¨áÔ∏è
(Downstream processing: Databricks, Synapse, Power BI)
```

---




## 2. **You need to copy files from an SFTP server to Azure Blob Storage and send an email to the manager if any file is missing. How would you implement this?**
‚úÖ Good scenario ‚Äî let me walk you through a **design** for this use case step by step:

> üìÑ **Goal**: Copy files from SFTP ‚Üí Azure Blob Storage, and if any expected file(s) are missing, send an email to the manager.

---

### üî∑ **High-level steps**

1Ô∏è‚É£ **Define the list of expected files**

* This could come from:

  * A static list configured in your pipeline (e.g., parameter or config file).
  * A dynamic list fetched from a database or metadata store.

---

2Ô∏è‚É£ **Pipeline implementation:**

We‚Äôll use **Azure Data Factory (ADF)** or **Synapse Pipelines** since they support SFTP, Blob Storage, and email notification.

---

### üìê **ADF Pipeline Design**

#### üìÇ 1. Get the list of actual files from SFTP

* Use a **Get Metadata activity** to list files in the SFTP folder.

  * Configure a Linked Service to connect to the SFTP server.
  * Use a Dataset pointing to the folder on SFTP.
  * The output will be an array of file names.

---

#### üìú 2. Compare actual vs expected

* Pass the list of expected files as a pipeline parameter or retrieve it from a Lookup activity.
* Use a **Set Variable** or a **Stored Procedure** / **Mapping Data Flow** / custom **Azure Function** / **Until activity** with an **If Condition** to compare the expected list and the actual list.

  * Identify which files are missing.

---

#### üì§ 3. Copy files

* For each file in the actual list:

  * Use a **ForEach activity** to loop over the files.
  * Inside the ForEach, use a **Copy activity**:

    * Source: SFTP file.
    * Sink: Azure Blob Storage container.

---

#### üìß 4. Notify manager if files are missing

* If any expected file is not present on the SFTP, trigger a **Web activity** to call:

  * Logic App
  * Power Automate
  * Or directly use the ADF **Send Email (Preview)** activity (if available in your region)
  * The email body can include which files were missing.

---

### üìÑ Example flow in ADF:

```
[Lookup: Expected file list]
          |
     [Get Metadata: SFTP folder file list]
          |
  [Compare lists: identify missing files]
          |
        /   \
[If no missing] [If missing]
   |               |
[ForEach: Copy]   [Send Email]
```

---

### üî∑ Notes:

‚úÖ Make sure the SFTP Linked Service has proper credentials (key or password).
‚úÖ Use a dynamic dataset in the Copy activity to handle different file names.
‚úÖ For comparison logic, you can use:

* Data Flow (with Left Anti Join between expected & actual).
* Stored Procedure in SQL.
* Custom Azure Function for complex logic.
  ‚úÖ For the email, if ADF native email isn‚Äôt available, a Logic App or Function is the usual choice.

---

### Sample tools:

| Step          | Tool/Activity                |
| ------------- | ---------------------------- |
| Expected list | Lookup or Pipeline parameter |
| Actual list   | Get Metadata                 |
| Compare lists | Data Flow or pipeline logic  |
| Copy files    | Copy activity in ForEach     |
| Send email    | Web activity ‚Üí Logic App     |

---

### üí° Example Email:

```
Subject: Missing files detected on SFTP

Hello Manager,

The following expected files were not found on the SFTP server during today‚Äôs run:
- file1.csv
- file2.csv

Please investigate.

Thanks,
Data Engineering Team
```

---


## 3. **You‚Äôre asked to ingest data from 50 different REST APIs with different authentication mechanisms. How would you make the pipeline reusable and dynamic?**

‚úÖ This is a **classic scenario** in data engineering where you need to ingest data from many REST APIs ‚Äî each with **different authentication mechanisms** ‚Äî and you want to build a **reusable & dynamic pipeline** rather than 50 hard-coded pipelines.

Here‚Äôs how you can design and implement it, step by step:

---

## üéØ Goals:

* Handle multiple APIs (50+).
* Support different authentication types (API key, OAuth2, Basic auth, etc.).
* Make the pipeline reusable ‚Äî no code changes when adding a new API.
* Make it dynamic ‚Äî parameters driven by configuration.

---

## üöÄ Solution Design

### 1Ô∏è‚É£ **Use a Metadata/Configuration-driven Approach**

Create a **configuration store** (in a database table, JSON, or YAML file) that contains all the details for each API.

Example schema for the config table:

| api\_name   | endpoint\_url                 | auth\_type  | auth\_details          | params | headers | schedule |
| ----------- | ----------------------------- | ----------- | ---------------------- | ------ | ------- | -------- |
| WeatherAPI  | `https://api.weather.com/...` | API\_KEY    | `{ "key": "abc123" }`  | `{‚Ä¶}`  | `{‚Ä¶}`   | daily    |
| TwitterAPI  | `https://api.twitter.com/...` | OAUTH2      | `{ "client_id": ‚Ä¶ }`   | `{‚Ä¶}`  | `{‚Ä¶}`   | hourly   |
| InternalAPI | `https://myapp.local/...`     | BASIC\_AUTH | `{ "user":‚Ä¶, "pwd":‚Ä¶}` | `{‚Ä¶}`  | `{‚Ä¶}`   | weekly   |

---

### 2Ô∏è‚É£ **Design the Pipeline**

You can build the pipeline in **Azure Data Factory**, **Databricks**, or any orchestration tool (Airflow, Prefect).
The key is:

* Read the configuration.
* Loop over each API config.
* Call the REST API with appropriate auth and params.
* Store the response.

---

### 3Ô∏è‚É£ **Implement Authentication Handlers**

Write reusable functions or modules to handle different `auth_type`:

* If `auth_type = API_KEY`: Inject the key into header or query string.
* If `auth_type = BASIC_AUTH`: Base64 encode user\:password.
* If `auth_type = OAUTH2`: Obtain token, refresh if expired, and attach to header.
* If `auth_type = NONE`: Just call.

You can implement these as Python functions, or ADF Web Activity dynamic expressions.

---

### 4Ô∏è‚É£ **Dynamic Pipeline in ADF (example)**

In **ADF**:

* Use a **Lookup activity** to read the configuration (from SQL table, Blob JSON, etc.)
* Use a **ForEach activity** to iterate over each API.
* Inside ForEach:

  * Use a **Web activity** (or Azure Function) to call the API.
  * Pass dynamic `url`, `headers`, `body`, etc. from the config.
  * Handle the response and save it (Blob, ADLS, SQL, etc.)
  * Optionally log the status.

---

### 5Ô∏è‚É£ **Dynamic Pipeline in Databricks (example)**

In **Databricks (PySpark)**:

* Read the config table into a DataFrame.
* Loop through rows:

  ```python
  import requests
  import base64

  for row in config_df.collect():
      url = row['endpoint_url']
      auth_type = row['auth_type']
      auth_details = json.loads(row['auth_details'])
      headers = json.loads(row['headers']) or {}
      params = json.loads(row['params']) or {}

      if auth_type == 'API_KEY':
          headers['Authorization'] = f"Bearer {auth_details['key']}"
      elif auth_type == 'BASIC_AUTH':
          token = base64.b64encode(f"{auth_details['user']}:{auth_details['pwd']}".encode()).decode()
          headers['Authorization'] = f"Basic {token}"
      elif auth_type == 'OAUTH2':
          # implement token retrieval & attach
          pass
      
      response = requests.get(url, headers=headers, params=params)
      # save response to storage
  ```

---

### 6Ô∏è‚É£ **Logging & Monitoring**

* Log success/failure for each API call.
* Capture response status & any error messages.
* You can store logs in a table or monitoring tool.

---

### 7Ô∏è‚É£ **Adding New APIs**

Once this framework is in place:

* To add a new API, just add a new record to the config table/file.
* No code or pipeline change required.

---

## üìù Benefits

‚úÖ Scalable: Easily supports >50 APIs.
‚úÖ Reusable: Single pipeline for all APIs.
‚úÖ Configurable: Changes driven by metadata.
‚úÖ Auditable: Logs everything centrally.

---

## Example Tech Stack:

| Task          | Tools                              |
| ------------- | ---------------------------------- |
| Orchestration | ADF, Airflow, Databricks workflows |
| Config Store  | Azure SQL DB, Cosmos DB, Blob JSON |
| API Calls     | ADF Web Activity / Python requests |
| Storage       | ADLS Gen2, Blob, SQL               |
| Monitoring    | Azure Monitor, Log Analytics       |

---


## 4. **If your source table doesn‚Äôt have a column that indicates updated/inserted records, how can you implement incremental load?**

‚úÖ Great question ‚Äî and a very common challenge in data engineering & ETL design!

If your **source table doesn‚Äôt have a column to indicate which records were inserted/updated (like a timestamp, last\_modified, or change\_flag)**, then you **cannot do a simple incremental load based on a watermark.**

But you *can still implement incremental loading* using one of the following strategies:

---

## üöÄ Options to Implement Incremental Load Without a Change Column

---

### 1Ô∏è‚É£ **Full Extract + Change Data Capture (CDC) Logic**

* Load the **entire source table** into a staging area (usually in your data lake or staging database).
* Compare the staging data with the target table to identify:

  * **New records** ‚Üí Not present in the target.
  * **Updated records** ‚Üí Present in the target but with different values.
  * **Unchanged records** ‚Üí Already in the target & identical.

You typically implement this using:

* A **hash or checksum** of each row to detect changes efficiently.
* Or by comparing all relevant columns directly.

**Steps:**

```sql
-- Step 1: Compute a hash of each record in source & target
SELECT id, md5(concat(col1, col2, col3, ‚Ä¶)) AS row_hash FROM source_table;
SELECT id, md5(concat(col1, col2, col3, ‚Ä¶)) AS row_hash FROM target_table;

-- Step 2: Join on `id` and compare hashes
-- If `id` not in target ‚Üí insert
-- If `id` in target & hash != ‚Üí update
-- If `id` in target & hash = ‚Üí skip
```

---

### 2Ô∏è‚É£ **Change Data Capture at the Database Level**

* Some databases (SQL Server, Oracle, MySQL, PostgreSQL) support CDC features or write-ahead logs (WAL) you can tap into.
* Tools like:

  * **SQL Server CDC/CT (Change Tracking)**
  * **Debezium**
  * **GoldenGate**
  * **Azure Data Factory with CDC in Mapping Data Flows**
    can capture changes at the transaction/log level ‚Äî even if your table itself doesn‚Äôt store `last_updated`.

---

### 3Ô∏è‚É£ **Triggers in Source Database**

If allowed, you can create a trigger on the source table to populate an auxiliary log table whenever rows are inserted/updated/deleted.
Your pipeline can then read from this log table to get the deltas.

---

### 4Ô∏è‚É£ **Snapshots + SCD (Slowly Changing Dimension) Type 2**

* Take regular **snapshots** of the table (daily, hourly, etc.).
* Compare today‚Äôs snapshot to the previous snapshot.
* Insert changes or maintain history (like SCD Type 2).

---

### 5Ô∏è‚É£ **External Tools**

If you‚Äôre using cloud tools like:

* **Azure Data Factory**,
* **Databricks**,
* **Fivetran**,
* **Snowflake streams**,
  you can often configure them to detect & load changes by comparing snapshots or using built-in CDC features.

---

## üìå Key Techniques Used

| Technique                      | Pros                        | Cons                         |
| ------------------------------ | --------------------------- | ---------------------------- |
| Full extract + hash comparison | Simple, no DB changes       | High data transfer & compute |
| CDC / Log-based                | Efficient, minimal overhead | Requires DB support & setup  |
| Triggers                       | Works on any DB             | Adds write overhead          |
| Snapshots + diff               | No DB change, keeps history | Storage intensive            |

---

## üë∑ Best Practice

‚úÖ If the source DB supports CDC ‚Üí use it.
‚úÖ If not, but data size is small ‚Üí use full extract + hash comparison.
‚úÖ If large & no CDC ‚Üí consider designing a lightweight trigger-based log.
‚úÖ Always try to push the responsibility for tracking changes to the **source system** if possible (add a `last_modified` column if feasible).

---



## 5. **Your pipeline needs to fetch yesterday‚Äôs sales data automatically. How would you parameterize and set up the pipeline for that?**


‚úÖ This is a very common **date-driven pipeline design question**, and a good answer shows you know how to make it **dynamic and reusable**.

Here is a clear explanation with **steps and an example** (using Azure Data Factory or Databricks, but the principles apply to any ETL tool):

---

### üéØ Goal:

Fetch **yesterday‚Äôs sales data** automatically every day ‚Äî for example, process sales where `sales_date = yesterday`.

---

## üöÄ Approach

### 1Ô∏è‚É£ **Parameterize the date**

* Define a **pipeline parameter** called `ProcessingDate`.
* You don‚Äôt hard-code it; instead, calculate it dynamically.

In Azure Data Factory (ADF), you can use the expression:

```sql
@formatDateTime(addDays(utcnow(), -1), 'yyyy-MM-dd')
```

üìå This computes yesterday‚Äôs date at runtime.

If you‚Äôre in Databricks:

```python
from datetime import datetime, timedelta
processing_date = (datetime.today() - timedelta(days=1)).strftime("%Y-%m-%d")
```

---

### 2Ô∏è‚É£ **Use this parameter in your source query/path**

* If you‚Äôre querying a database:

```sql
SELECT * 
FROM sales 
WHERE sales_date = '@{pipeline().parameters.ProcessingDate}'
```

* If you‚Äôre reading from files (like `sales_YYYY-MM-DD.csv`):
  you can build the path dynamically:

```
/data/sales/sales_@{pipeline().parameters.ProcessingDate}.csv
```

---

### 3Ô∏è‚É£ **Schedule the pipeline**

* Use a trigger to **run daily at a set time**, e.g., 2 AM.
* No manual intervention is needed since the date is calculated dynamically.

---

## üî∑ Optional Enhancements

‚úÖ Add a fallback: If the parameter is supplied manually, use it; else default to yesterday.
ADF example:

```sql
@if(equals(pipeline().parameters.ProcessingDate, ''), 
    formatDateTime(addDays(utcnow(), -1), 'yyyy-MM-dd'), 
    pipeline().parameters.ProcessingDate)
```

‚úÖ Add validation to ensure data for the date exists before processing.

---

### üìã Example Summary Table

| Step                           | What you do                                            |
| ------------------------------ | ------------------------------------------------------ |
| Define parameter               | `ProcessingDate`                                       |
| Default value (ADF expression) | `@formatDateTime(addDays(utcnow(), -1), 'yyyy-MM-dd')` |
| Use parameter in query/path    | `WHERE sales_date = '@ProcessingDate'` or file path    |
| Schedule pipeline              | Daily trigger (e.g., at 2 AM)                          |


---

### üî∑ Data Transformation & Data Flow Scenarios

## 6. **You have two datasets (customer & orders) in Blob Storage that need to be joined on `CustomerID`, aggregated, and written into Azure SQL Database. How would you build this?**

## 7. **How would you implement Slowly Changing Dimension (SCD) Type 2 logic in ADF Data Flows?**

## 8. **If the source file contains invalid/malformed rows, how would you handle and log them?**

## 9. **Your pipeline needs to convert files from CSV to Parquet and partition them by date before storing in ADLS. How would you achieve this?**

---

### üî∑ Orchestration & Monitoring Scenarios

## 10. **How would you design a pipeline that runs every day at 2AM, but only if the upstream system has deposited a new file in Blob Storage?**

## 11. **How would you retry a failed activity only 3 times and send a Teams notification if it still fails?**

## 12. **You need to execute pipelines in a specific order ‚Äî Pipeline A ‚Üí Pipeline B ‚Üí Pipeline C, but only if A & B are successful. How would you design this?**

## 13. **How would you implement a mechanism to log every pipeline run status, start & end time, and activity outcomes into a SQL table for auditing?**

---

### üî∑ Performance & Optimization Scenarios

## 14. **What techniques would you use to optimize the performance of a copy activity transferring millions of rows?**

## 15. **How would you design your pipeline to scale when processing 1TB of daily data without failing?**

---

### üî∑ Dynamic & Parameterized Pipelines

## 16. **You have to load data from 100 tables of a database into their respective folders in ADLS. How would you build a single dynamic pipeline to handle this?**

## 17. **Your pipeline needs to read the schema of incoming files dynamically and load them to a staging table. How would you handle schema drift?**

---

### üî∑ Security & Access Scenarios

## 18. **If you need to securely connect to an on-prem SQL Server via Self-Hosted Integration Runtime, what steps would you take?**

## 19. **How would you ensure that sensitive information (like passwords or API keys) are not exposed in your pipelines?**

---

### üî∑ Advanced/Real-Time Scenarios

## 20. **You‚Äôre asked to implement a pipeline that triggers on arrival of a file, transforms it, and updates Power BI datasets in near real-time. How would you design it?**

## 21. **You have to process and validate thousands of small files arriving every hour and merge them into a single Parquet file. How would you build this?**

---

## üìå Bonus ‚Äî Troubleshooting & Best Practices

## 22. **A pipeline that was running fine yesterday is now failing with a timeout error when writing to SQL Database. How would you debug it?**

## 23. **What are some best practices you follow for naming conventions, folder structure, and reusability in ADF?**

## 24. **How would you test your pipelines before moving them to production?**

## 25. **What would you do if the dataset schema at the source changed suddenly and broke your pipeline?**

---
