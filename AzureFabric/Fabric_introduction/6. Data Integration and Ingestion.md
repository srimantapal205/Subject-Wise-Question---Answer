## ‚öôÔ∏è **Data Integration and Ingestion in Microsoft Fabric**

### üß≠ **Introduction**

In any data ecosystem, the **first step** is to **bring data into the platform** ‚Äî this is known as **data ingestion**.
In Microsoft Fabric, **Data Integration and Ingestion** are primarily handled using **Data Factory**, which provides:

* **Pipelines** for data movement and orchestration
* **Dataflows** for transformation and preparation
* **Scheduling and incremental loads** for automation and efficiency

Fabric‚Äôs Data Factory combines the **power of Azure Data Factory (ADF)** with the **simplicity of Power BI Dataflows**, creating a unified SaaS experience for integrating data from multiple sources.

---

## üîπ **1. Pipelines and Dataflows**

### **1.1 Pipelines**

A **Pipeline** in Microsoft Fabric is a **visual workflow** that orchestrates data movement and transformation tasks.
It defines how data is extracted from sources, transformed, and loaded into destinations (ETL/ELT).

#### **Key Features**

* Drag-and-drop visual designer
* Integration with **hundreds of connectors** (SQL, Blob, Salesforce, etc.)
* **Data movement**, **copy activities**, and **control flow** (loops, conditions)
* **Triggers and scheduling** for automation

#### **Conceptual Diagram: Data Pipeline Flow**

```
          +--------------------+
          | Data Sources       |
          | (SQL, Blob, APIs)  |
          +--------------------+
                     ‚îÇ
                     ‚ñº
           +----------------------+
           | Fabric Data Pipeline |
           +----------------------+
              ‚îÇ         ‚îÇ
      +---------------+ +----------------+
      | Copy Activity | | Transformations|
      +---------------+ +----------------+
              ‚îÇ
              ‚ñº
       +-----------------+
       | Fabric Lakehouse|
       | or Data Warehouse|
       +-----------------+
```

---

### **1.2 Dataflows**

**Dataflows** in Microsoft Fabric are **low-code, Power Query‚Äìbased tools** used for **data transformation and preparation**.
They‚Äôre ideal for **business users or analysts** who want to clean, shape, and enrich data before loading it into Fabric items like Lakehouses or Warehouses.

#### **Key Capabilities**

* GUI-based transformation (Power Query interface)
* Supports connectors to databases, files, web APIs, etc.
* Can output to **Lakehouses**, **Warehouses**, or **Power BI datasets**
* Reusable across multiple reports and pipelines

#### **When to Use**

| Use Case                              | Recommended Tool |
| ------------------------------------- | ---------------- |
| Complex ETL with orchestration        | **Pipelines**    |
| Interactive data shaping and cleaning | **Dataflows**    |

---

## üîπ **2. Copy Activity and Data Transformations**

### **2.1 Copy Activity**

The **Copy Activity** is the **core operation** in a Fabric pipeline ‚Äî it moves data from a **source** to a **destination**.

#### **Example Use Case**

Copy customer data from an **on-premises SQL Server** to a **Fabric Lakehouse** for analytics.

#### **Steps**

1. Define source (SQL Server).
2. Define destination (Lakehouse).
3. Map schema and column transformations.
4. Execute pipeline ‚Äî data is copied and stored in OneLake.

#### **Key Settings**

* **Batch size** and **parallelism** for performance tuning.
* **Fault tolerance** and **retry policies**.
* **Data mapping** for schema alignment.

---

### **2.2 Data Transformations**

Fabric supports two transformation types:

1. **Power Query transformations** (for dataflows) ‚Äî e.g., removing nulls, merging columns, filtering data.
2. **Pipeline transformations** (for data engineers) ‚Äî using **Dataflow Gen2** or **Spark notebooks** for advanced ETL/ELT.

#### **Example Transformations**

* Split ‚ÄúFullName‚Äù into ‚ÄúFirstName‚Äù and ‚ÄúLastName‚Äù.
* Replace missing values with defaults.
* Join multiple datasets (Sales + Customers).
* Aggregate sales by region.

---

## üîπ **3. Incremental Loading and Scheduling**

### **3.1 Incremental Loading**

Instead of reloading an entire dataset every time, **incremental loading** only brings **new or changed records**, saving time and resources.

#### **Approaches in Fabric**

* **Watermark column method:**
  Use a timestamp or ID column to detect changes.
* **Change Data Capture (CDC):**
  Capture only the delta changes from the source system.
* **Partition-based approach:**
  Load specific partitions (e.g., by date).

#### **Example**

A Sales table has a `LastModifiedDate` column.
Each pipeline run loads only records with a `LastModifiedDate` later than the last run time.

#### **Pseudocode Logic**

```
SELECT * FROM Sales
WHERE LastModifiedDate > @LastLoadTime
```

---

### **3.2 Scheduling**

Pipelines in Fabric can be **scheduled** to run automatically using **triggers**.
This enables automation of daily, hourly, or event-based data refreshes.

#### **Trigger Types**

* **Schedule trigger:** Run on a predefined schedule (e.g., every 24 hours).
* **Event trigger:** Run when new data arrives in a source.
* **Manual trigger:** Run on-demand by a user.

#### **Example**

A pipeline runs **every night at 2 AM**, copying daily sales data from SQL to the Lakehouse.

---

### **Diagram: Incremental Load and Scheduling**

```
+-------------------+
|  Source Database  |
| (SQL Server)      |
+-------------------+
          ‚îÇ
          ‚ñº
+----------------------+
| Fabric Pipeline      |
|  - Copy Activity     |
|  - Filter: > LastRun |
|  - Schedule: Daily   |
+----------------------+
          ‚îÇ
          ‚ñº
+-------------------+
| Fabric Lakehouse  |
| (OneLake Storage) |
+-------------------+
```

---

## üîπ **4. Integration with Azure Data Factory (ADF)**

### **Overview**

Fabric‚Äôs Data Factory is **built on the same foundation** as **Azure Data Factory**, but it is **natively integrated** with the Fabric ecosystem.

### **Comparison Table**

| Feature            | **Azure Data Factory (ADF)** | **Fabric Data Factory**                |
| ------------------ | ---------------------------- | -------------------------------------- |
| **Type**           | PaaS (Platform as a Service) | SaaS (Software as a Service)           |
| **Integration**    | Standalone, Azure-based      | Fully integrated with Fabric & OneLake |
| **Management**     | Requires Azure setup         | Managed automatically within Fabric    |
| **Security**       | Azure RBAC, Key Vault        | Microsoft Entra ID, Fabric RBAC        |
| **Output**         | Azure Storage, SQL DB        | Fabric Lakehouse, Warehouse, OneLake   |
| **User Interface** | Azure Portal                 | Fabric Web Interface                   |
| **Ideal Users**    | Data Engineers               | Business and Data Teams                |

### **Interoperability**

Fabric Data Factory can **reuse existing ADF pipelines** through **copy-paste migration** or **REST APIs**, and both platforms share:

* **Same connectors**
* **Pipeline JSON format**
* **Monitoring capabilities**

---

### **Example Scenario ‚Äì End-to-End Integration**

A retail company wants to automate their data ingestion:

1. **ADF** copies historical data from on-prem SQL to Azure Blob.
2. **Fabric Data Factory** pipelines load new daily data into **OneLake**.
3. **Dataflows** clean and prepare the data.
4. **Power BI** visualizes daily sales metrics.

Together, they provide a **complete hybrid ingestion solution**.

---

## üß† **Summary**

| **Concept**             | **Description**                                          |
| ----------------------- | -------------------------------------------------------- |
| **Pipelines**           | Orchestrate data ingestion and transformation workflows. |
| **Dataflows**           | Low-code data transformation using Power Query.          |
| **Copy Activity**       | Moves data from source to destination.                   |
| **Transformations**     | Clean, shape, and enrich data before analysis.           |
| **Incremental Loading** | Loads only new or updated records for efficiency.        |
| **Scheduling**          | Automates pipeline execution at specific intervals.      |
| **ADF Integration**     | Shares connectors and concepts with Azure Data Factory.  |

---

## üß≠ **Overall Diagram: Fabric Data Integration Architecture**

```
              +-----------------------------+
              |     Data Sources            |
              |  (SQL, Blob, APIs, SaaS)    |
              +-------------+---------------+
                            ‚îÇ
                            ‚ñº
             +-----------------------------+
             |   Fabric Data Factory        |
             |  - Pipelines (ETL)          |
             |  - Dataflows (Transform)    |
             +-------------+---------------+
                            ‚îÇ
                            ‚ñº
               +-------------------------+
               |   OneLake (Unified Data)|
               +-------------------------+
                            ‚îÇ
                            ‚ñº
             +-----------------------------+
             | Power BI / Data Activator   |
             | (Visualization & Automation)|
             +-----------------------------+
```

---

## ‚ùì **Practice Questions & Answers**

**Q1.** What is the purpose of Data Pipelines in Microsoft Fabric?
**A1.** Pipelines are used to orchestrate data movement and transformation workflows across multiple sources and destinations.

**Q2.** What is a Dataflow in Fabric?
**A2.** A Dataflow is a Power Query‚Äìbased tool that allows users to clean and transform data in a low-code environment.

**Q3.** What is the function of a Copy Activity?
**A3.** The Copy Activity moves data from a source to a destination, supporting schema mapping, parallelism, and fault tolerance.

**Q4.** What are two main methods for incremental loading?
**A4.** Using a **watermark column** or **Change Data Capture (CDC)** to detect and load only changed data.

**Q5.** How can pipelines be scheduled in Fabric?
**A5.** By creating triggers that execute pipelines at specific times, intervals, or events.

**Q6.** What is the key difference between Azure Data Factory and Fabric Data Factory?
**A6.** Azure Data Factory is a PaaS service for advanced integration, while Fabric Data Factory is a SaaS solution integrated within Microsoft Fabric.

**Q7.** Can Fabric reuse Azure Data Factory connectors?
**A7.** Yes, Fabric Data Factory shares the same connectors and integration logic as Azure Data Factory.

**Q8.** Give one benefit of incremental loading.
**A8.** It reduces processing time and resource usage by loading only new or updated data instead of reloading everything.

---
