## üß± **Data Engineering in Microsoft Fabric**

### üß≠ **Introduction**

**Data Engineering** in Microsoft Fabric focuses on building **scalable, high-performance data systems** for storing, transforming, and analyzing large volumes of data.

The core objective is to:

* Ingest raw data into **Lakehouses**
* Organize and manage it efficiently using **Delta tables**
* Perform **data transformations** using **Notebooks (Python, SQL, or Spark)**
* Prepare clean, structured data for downstream analytics (Data Science, Power BI, etc.)

Microsoft Fabric provides a **unified data engineering environment**, tightly integrated with **OneLake**, **Data Factory**, and **Power BI** ‚Äî reducing friction between ingestion, transformation, and visualization.

---

## üîπ **1. Creating Lakehouses**

### **What is a Lakehouse?**

A **Lakehouse** in Microsoft Fabric combines the capabilities of:

* **Data Lakes** (flexible, large-scale storage for raw data)
* **Data Warehouses** (structured querying and performance)

It supports **structured, semi-structured, and unstructured data**, and stores everything in **Delta Lake format** ‚Äî enabling ACID transactions and efficient data versioning.

---

### **Key Components of a Lakehouse**

| Component        | Description                                                          |
| ---------------- | -------------------------------------------------------------------- |
| **Files area**   | Stores raw and semi-structured files (CSV, JSON, Parquet, etc.)      |
| **Tables area**  | Stores structured data as Delta tables                               |
| **SQL Endpoint** | Allows querying data using T-SQL                                     |
| **Notebooks**    | Used for data exploration, transformation, and analysis              |
| **Shortcuts**    | Virtual links to data stored in other workspaces or external sources |

---

### **Diagram: Lakehouse Architecture**

```
             +-------------------------------------------+
             |             Microsoft Fabric              |
             +-------------------------------------------+
                                ‚îÇ
                 +-----------------------------------+
                 |           Lakehouse               |
                 +-----------------------------------+
                  ‚îÇ               ‚îÇ               ‚îÇ
          +--------------+ +-------------+ +--------------+
          | Files (Raw)  | | Tables (Delta)| | Notebooks   |
          +--------------+ +-------------+ +--------------+
                                ‚îÇ
                     +----------------------+
                     |   SQL Endpoint       |
                     +----------------------+
```

---

### **Steps to Create a Lakehouse**

1. Go to your **Fabric workspace**.
2. Click **‚Äú+ New‚Äù ‚Üí Lakehouse**.
3. Provide a name and optional description.
4. Once created, the Lakehouse automatically links to **OneLake**.
5. You can now **ingest data** using:

   * Data Factory pipelines
   * Upload files directly
   * Create tables via SQL or Notebooks

---

### **Example**

**Scenario:**
A retail company wants to store and analyze daily sales data.

Steps:

1. Create a **Lakehouse** called `SalesLakehouse`.
2. Ingest CSV files of sales transactions via **Fabric Data Factory pipeline**.
3. Convert raw CSV files into a structured **Delta table** (`SalesData`).
4. Analyze and visualize the cleaned data in **Power BI**.

---

## üîπ **2. Managing Delta Tables**

### **What are Delta Tables?**

**Delta tables** are enhanced **Parquet-based tables** that support:

* **ACID transactions**
* **Time travel**
* **Schema evolution**
* **Efficient read/write performance**

They are the **default storage format** for Fabric Lakehouses.

---

### **Key Features**

| Feature               | Description                                         |
| --------------------- | --------------------------------------------------- |
| **ACID transactions** | Ensures reliable, consistent data operations.       |
| **Schema evolution**  | Automatically adds new columns as data evolves.     |
| **Time travel**       | Query data as of a previous version.                |
| **Version control**   | Tracks every change to the dataset.                 |
| **Optimized storage** | Automatically compacts small files for performance. |

---

### **Common Delta Table Operations**

| Operation       | Example                                            |
| --------------- | -------------------------------------------------- |
| **Create**      | `CREATE TABLE Sales AS SELECT * FROM source_data;` |
| **Insert**      | `INSERT INTO Sales VALUES (...);`                  |
| **Update**      | `UPDATE Sales SET Total = 100 WHERE ID = 1;`       |
| **Delete**      | `DELETE FROM Sales WHERE Region = 'West';`         |
| **Time Travel** | `SELECT * FROM Sales VERSION AS OF 3;`             |

---

### **Example**

Suppose you have a Delta table named **Sales**:

```sql
CREATE TABLE Sales AS
SELECT * FROM read_csv("Files/DailySales.csv");
```

Now, if a new column (`Discount`) is added in future files, Fabric automatically updates the schema.

You can check table history:

```sql
DESCRIBE HISTORY Sales;
```

---

### **Diagram: Delta Table Lifecycle**

```
+-------------------+
|  Raw Data (CSV)   |
+---------+---------+
          |
          ‚ñº
+-------------------+
|  Delta Table      |   (ACID, versioned)
|  'SalesData'      |
+---------+---------+
          |
          ‚ñº
+-------------------+
| Power BI / Reports|
+-------------------+
```

---

## üîπ **3. Notebooks (Python, SQL, Spark)**

### **Purpose**

Notebooks in Fabric are **interactive environments** used for:

* Data exploration and visualization
* ETL/ELT transformations
* Machine learning
* Advanced analytics

Fabric Notebooks support **Python**, **SQL**, and **Spark (PySpark)** ‚Äî all integrated with the Lakehouse and OneLake.

---

### **Notebook Interfaces**

* **Python Notebook:** For data manipulation (pandas, pyspark)
* **SQL Notebook:** For querying tables directly
* **Spark Notebook:** For distributed data processing at scale

---

### **Example Notebook Workflow**

**Scenario:**
You want to analyze top-performing products from your `SalesData` Delta table.

**SQL Cell**

```sql
SELECT ProductName, SUM(TotalSales) AS Total
FROM SalesData
GROUP BY ProductName
ORDER BY Total DESC
LIMIT 10;
```

**Python Cell**

```python
df = spark.sql("SELECT * FROM SalesData WHERE Region='East'")
display(df.describe())
```

**Spark Cell**

```python
from pyspark.sql.functions import col
df = spark.read.format("delta").load("/Tables/SalesData")
df_filtered = df.filter(col("TotalSales") > 1000)
df_filtered.write.format("delta").mode("overwrite").save("/Tables/HighSales")
```

---

### **Diagram: Notebook Integration**

```
       +----------------------+
       |   Fabric Notebook    |
       | (Python / SQL / Spark)|
       +----------+-----------+
                  |
          +---------------+
          |  Lakehouse    |
          |  (Delta Tables)|
          +---------------+
                  |
          +----------------+
          |  OneLake (Storage) |
          +----------------+
```

---

## üîπ **4. Data Transformation and Processing**

### **Overview**

Data transformations in Fabric involve **cleaning**, **enriching**, and **aggregating** data using Notebooks, Spark, or SQL queries.

### **Common Transformation Tasks**

| Task                 | Example                                   |
| -------------------- | ----------------------------------------- |
| **Data Cleaning**    | Removing duplicates or null values        |
| **Aggregation**      | Summing sales by region or category       |
| **Joining**          | Combining Sales and Customer tables       |
| **Deriving Columns** | Calculating profit = Sales ‚Äì Cost         |
| **Filtering**        | Selecting data from a specific date range |

---

### **Example ‚Äì Data Cleaning and Aggregation**

```python
from pyspark.sql.functions import col, sum as _sum

# Load data
df = spark.read.format("delta").load("/Tables/SalesData")

# Clean and aggregate
cleaned_df = df.dropna().filter(col("Region") != "Unknown")
aggregated_df = cleaned_df.groupBy("Region").agg(_sum("TotalSales").alias("RegionalSales"))

# Save results
aggregated_df.write.format("delta").mode("overwrite").save("/Tables/RegionalSales")
```

This prepares summarized, clean data ready for Power BI visualization.

---

### **Diagram: Data Engineering Flow in Fabric**

```
[Raw Data Sources]
       ‚îÇ
       ‚ñº
[Data Factory Pipelines]
       ‚îÇ
       ‚ñº
[Fabric Lakehouse]
   ‚îú‚îÄ‚îÄ Files (Raw)
   ‚îú‚îÄ‚îÄ Delta Tables (Structured)
   ‚îî‚îÄ‚îÄ Notebooks (Transform & Analyze)
       ‚îÇ
       ‚ñº
[Cleaned Tables ‚Üí Power BI / Data Science]
```

---

## üß† **Summary**

| Concept                 | Description                                                          |
| ----------------------- | -------------------------------------------------------------------- |
| **Lakehouse**           | Unified storage for structured and unstructured data in Delta format |
| **Delta Tables**        | Versioned, ACID-compliant tables with schema evolution               |
| **Notebooks**           | Interactive tools for transformation using Python, SQL, or Spark     |
| **Data Transformation** | Cleaning and aggregating data for downstream analytics               |
| **Integration**         | Fully integrated with OneLake and Power BI                           |

---

## ‚ùì **Practice Questions & Answers**

**Q1.** What is the purpose of a Lakehouse in Microsoft Fabric?
**A1.** A Lakehouse unifies data lake and warehouse capabilities, allowing structured and unstructured data storage and analytics in one place.

**Q2.** What is a Delta Table?
**A2.** A Delta Table is a Parquet-based storage format supporting ACID transactions, schema evolution, and time travel in Fabric Lakehouses.

**Q3.** How are Delta tables beneficial for data engineers?
**A3.** They provide reliability, automatic schema updates, version control, and efficient performance for big data operations.

**Q4.** Which languages are supported in Fabric Notebooks?
**A4.** Python, SQL, and Spark (PySpark).

**Q5.** What are some common data transformation tasks in Fabric?
**A5.** Data cleaning, aggregation, joins, filtering, and deriving new metrics.

**Q6.** How can a Lakehouse be queried?
**A6.** Using its built-in **SQL endpoint** or via **Notebooks**.

**Q7.** What is the main advantage of using Delta format in Fabric Lakehouses?
**A7.** It supports ACID transactions and enables scalable, consistent data processing with version control.

**Q8.** Can Notebooks directly read and write Delta tables?
**A8.** Yes, Notebooks can read from and write to Delta tables using Spark APIs.


