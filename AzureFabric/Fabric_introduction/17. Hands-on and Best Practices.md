#  **17. Hands-On and Best Practices**

---

## ðŸŸ¦ **A. Step-by-Step Hands-On Tutorials**

### ðŸ“Œ **Tutorial 1: Build an End-to-End Data Pipeline in Fabric**

**Goal:** Ingest â†’ Transform â†’ Model â†’ Report

| Step | Fabric Component | Action                                                                            |
| ---- | ---------------- | --------------------------------------------------------------------------------- |
| 1    | Data Pipeline    | Create pipeline â†’ Add Copy Activity â†’ Ingest CSV/API/SQL data to Lakehouse Bronze |
| 2    | Lakehouse        | Verify files â†’ Auto-create Delta tables                                           |
| 3    | Notebook         | Write PySpark logic â†’ Clean, dedupe, add business rules â†’ Write to Silver         |
| 4    | SQL              | Create fact + dimension tables â†’ Store Gold layer                                 |
| 5    | Semantic Model   | Auto-create model â†’ Add relationships & DAX KPIs                                  |
| 6    | Power BI         | Build dashboard using **Direct Lake mode**                                        |
| 7    | Schedule         | Set pipeline trigger â†’ Automatic daily refresh                                    |

ðŸ”¹ **No data movement between systems** â€” everything stays in **OneLake**
ðŸ”¹ **Direct Lake eliminates dataset refresh overhead**

---

### ðŸ“Œ **Tutorial 2: Real-Time Dashboard in Fabric**

| Step | Component      | Action                                       |
| ---- | -------------- | -------------------------------------------- |
| 1    | KQL Database   | Create table for streaming                   |
| 2    | Eventstream    | Attach IoT/Web App/EventHub as Source        |
| 3    | Transformation | Add splitting, enrichment, parsing rules     |
| 4    | Destination    | Output to Lakehouse + KQL DB                 |
| 5    | Power BI       | Connect using DirectQuery to real-time table |
| 6    | Alerts         | Optional: Push alerts to Teams/Email         |

ðŸ“Œ Ideal for **IoT telemetry, fraud detection, clickstream analytics**

---

### ðŸ“Œ **Tutorial 3: Predictive Analytics / ML in Fabric**

| Step | Component       | Action                                            |
| ---- | --------------- | ------------------------------------------------- |
| 1    | Notebook        | Explore Lakehouse Gold dataset                    |
| 2    | ML runtime      | Build & train model (prophet / sklearn / XGBoost) |
| 3    | MLflow          | Track experiment metrics                          |
| 4    | Model Registry  | Register model                                    |
| 5    | Batch inference | Score new data â†’ write results to Lakehouse       |
| 6    | Power BI        | Visualize predictions in KPIs                     |

ðŸ“Œ No need for exporting data to Azure ML â€” everything runs inside Fabric.

---

---

## ðŸŸ¦ **B. Common Troubleshooting in Microsoft Fabric**

| Issue                            | Root Cause                         | Fix                                                      |
| -------------------------------- | ---------------------------------- | -------------------------------------------------------- |
| Power BI Direct Lake not working | Table not in Delta format          | Ensure table = Delta; enable V-Order optimization        |
| Copy Activity fails              | Timeout / credentials              | Increase timeout; re-enter keyvault/pipeline credentials |
| Notebook slow                    | Small compute or unoptimized join  | Use partitioning + broadcast joins + cluster config      |
| Lakehouse SQL table not visible  | Cache sync delay                   | Refresh lakehouse SQL endpoint                           |
| Warehouse query slow             | Skewed distribution or large scans | Use hints, clustering keys, star schema                  |
| Pipelines queuing                | Low capacity                       | Use autoscale or distribute workloads across capacities  |
| Semantic model refresh failure   | Dataset too large in Import mode   | Switch to **Direct Lake mode**                           |

ðŸ”¹ Always check: **Activity Hub â†’ Run Logs â†’ Error Code â†’ Root Cause Suggestions**

---

---

## ðŸŸ¦ **C. Performance Optimization Best Practices**

### ðŸ”¹ **Lakehouse / Delta Optimization**

| Category            | Best Practice                                                  |
| ------------------- | -------------------------------------------------------------- |
| Storage             | Use **Delta tables** across Bronze â†’ Silver â†’ Gold             |
| File Layout         | Use **auto-optimize + V-Order**                                |
| Writes              | Use **merge + upsert** only in Gold layer                      |
| Prevent Small Files | Use **OPTIMIZE + VACUUM**                                      |
| Partitioning        | Partition by **date/business key** but not too many partitions |

---

### ðŸ”¹ **Data Engineering (PySpark) Optimization**

| Technique                          | Benefit                |
| ---------------------------------- | ---------------------- |
| Cache small lookup tables          | Faster joins           |
| Use broadcast joins                | Reduce shuffle         |
| Drop columns early                 | Reduce memory pressure |
| Use checkpoints                    | Prevent DAG expansion  |
| Save intermediate results in Delta | Faster fault-tolerance |

---

### ðŸ”¹ **Warehouse Optimization**

| Technique                     | Benefit                     |
| ----------------------------- | --------------------------- |
| Use star schema               | Best for analytics          |
| Cluster by key columns        | Reduces scan time           |
| Avoid SELECT *                | Improve memory + I/O        |
| Prefer numeric surrogate keys | Better joins than strings   |
| Materialized views            | Pre-compute expensive logic |

---

### ðŸ”¹ **Power BI Optimization**

| Setting      | Best Practice                                  |
| ------------ | ---------------------------------------------- |
| Storage mode | **Direct Lake** for large models               |
| Measures     | Optimize DAX â€” remove iterators where possible |
| Star schema  | Always recommended                             |
| Visuals      | Avoid > 30 visuals per page                    |
| Aggregations | Use summary tables for high-volume fact tables |

---

### ðŸ”¹ **Capacity & Cost Optimization**

| Method                  | Description                            |
| ----------------------- | -------------------------------------- |
| Autoscale               | Prevent throttling during peak         |
| Pause A-SKU             | Stop compute when not needed           |
| Isolate heavy workloads | Use separate capacities for PROD / DEV |
| Schedule refreshes      | Avoid peak hours                       |

---

---

# ðŸŸ£ Quick Cheat Sheet (For Interviews)

| Area          | Golden Rule                                                  |
| ------------- | ------------------------------------------------------------ |
| Data Modeling | Always build **star schema** in Gold layer                   |
| Storage       | Use **Delta Lake + V-Order** for performance                 |
| Analytics     | Use **Direct Lake mode** for Power BI                        |
| Governance    | Use **workspace + RLS + capacity roles**                     |
| Cost          | **Autoscale + capacity separation**                          |
| AI            | Run ML **inside Fabric**, push predictions back to Lakehouse |

---

## ðŸ§  Interview-Style Practice Questions

| Question                                              | Best Answer Highlight                                                            |
| ----------------------------------------------------- | -------------------------------------------------------------------------------- |
| How do you prevent slow pipelines in Fabric?          | Auto-optimize Delta, broadcast joins, cache dimension tables, optimize file size |
| Why Direct Lake is better than Import mode?           | Real-time + high performance + no data refresh                                   |
| How do you handle slowly changing dimensions?         | Use MERGE in Gold layer + surrogate keys                                         |
| What to do when warehouse query performance degrades? | Review clustering keys, avoid wide joins, enable materialized views              |

---
