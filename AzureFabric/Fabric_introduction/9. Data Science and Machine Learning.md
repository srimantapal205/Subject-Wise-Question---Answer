#  **9. Data Science & Machine Learning in Microsoft Fabric**

Microsoft Fabric provides a **unified environment** for building, training, deploying, and consuming machine learning (ML) models **without moving data outside the platform**.

It integrates:

* **Lakehouse**
* **Notebooks (Spark / Python)**
* **ML Runtime**
* **MLflow for model tracking**
* **Azure Machine Learning (optional for advanced MLOps)**

---

## üü¶ **1. Notebooks and ML Models in Fabric**

Fabric supports multi-language notebooks:

* Python
* PySpark
* SQL
* Markdown
* Native visualizations

### Notebook Capabilities

| Capability  | Description                                          |
| ----------- | ---------------------------------------------------- |
| Data access | Direct access to Lakehouse tables & files            |
| Compute     | Distributed Spark cluster                            |
| Libraries   | sklearn, XGBoost, Prophet, TensorFlow, PyTorch, etc. |
| MLflow      | Track experiments & performance                      |
| Scheduling  | Run notebooks via Fabric pipelines                   |
| Autologging | Automatically capture metrics during ML training     |

### Typical Notebook Workflow

```
Load data from Lakehouse ‚Üí Preprocess ‚Üí Feature engineering ‚Üí Train model ‚Üí Evaluate ‚Üí Log with MLflow ‚Üí Register model
```

### Example Model Types

* Classification (fraud, churn)
* Regression (demand forecasting, price optimization)
* Time series (sales forecast)
* Recommendation systems

---

## üü¶ **2. Integration with Azure Machine Learning**

Although Fabric can run ML **independently**, integration with Azure ML is available for:

* GPU-based high-performance compute
* AutoML & hyperparameter tuning
* Advanced MLOps (CI/CD pipelines)
* Deployment of models as REST endpoints

### Integration Workflow

```
Fabric Lakehouse ‚Üí Train model in Azure ML ‚Üí Register ‚Üí Deploy endpoint ‚Üí Score back into Lakehouse ‚Üí Power BI
```

### When to use Azure ML instead of Fabric ML runtime

| Use Case                                 | Platform |
| ---------------------------------------- | -------- |
| Standard ML on tabular data              | Fabric   |
| Large deep learning / NLP / LLM training | Azure ML |
| On-premises or enterprise CI/CD          | Azure ML |
| Model scoring inside data pipelines      | Fabric   |

---

## üü¶ **3. Model Training & Scoring (Inference)**

### üîπ Training (Building the model)

Steps:

1. Load dataset (Gold table) from Lakehouse
2. Split into train/test
3. Train the model (sklearn / Spark ML / XGBoost)
4. Evaluate metrics
5. Track run in **MLflow**
6. Register best model in **Model Catalog**

### üîπ Scoring (Generating predictions)

Fabric supports **two scoring methods**:

| Method                     | Use Case                               |
| -------------------------- | -------------------------------------- |
| Batch Scoring              | Predict on large datasets periodically |
| Real-Time / Online Scoring | Predict instantly with API endpoint    |

#### Batch Scoring Example

```
Gold table ‚Üí Inference Notebook ‚Üí Write predictions to Gold ‚Üí Power BI dashboard
```

#### Real-Time Scoring Example

```
Model deployed ‚Üí called via REST ‚Üí result stored or shown in application/UI ‚Üí real-time dashboard
```

### Integration with Power BI

Predictions can be visualized:

* As fields in a semantic model
* As KPI metrics
* In real-time dashboards through KQL streaming

---

## üß† Real-World ML Use Case in Fabric

### Business Example: Customer Churn Prediction

| Stage                              | Fabric Component         |
| ---------------------------------- | ------------------------ |
| Ingest CRM + transactional history | Pipelines                |
| Prepare Bronze/Silver data         | Notebooks                |
| Gold layer dimensional model       | Lakehouse / Warehouse    |
| Train model                        | Fabric ML Notebook       |
| Register + track                   | MLflow                   |
| Score monthly                      | Batch inference notebook |
| Deliver insights to business       | Power BI dashboard       |

Power BI dashboard can show:

* Churn probability
* At-risk customer list
* Recommended retention actions

---

## üîÑ End-to-End ML Workflow Summary

```
Raw data ‚Üí Bronze ‚Üí Silver ‚Üí Gold ‚Üí Train Model ‚Üí Track/Store Model ‚Üí Score ‚Üí Write predictions ‚Üí Dashboards
```

All without moving data outside **OneLake**.

---

# üü£ Best Practices for ML in Fabric

| Area          | Best Practice                                          |
| ------------- | ------------------------------------------------------ |
| Data Quality  | Train using Gold layer only                            |
| Performance   | Cache features & downsample before training if needed  |
| Feature Store | Reuse Silver/Gold features across models               |
| Tracking      | Use MLflow for metrics, parameters, and versions       |
| Deployment    | Register best model ‚Üí automate scoring using pipelines |
| Monitoring    | Track scoring time, drift, and prediction distribution |
| Security      | Restrict access to model and scoring outputs           |

---

# üß† Interview Questions with Sample Answers

### **Q1: How do Data Scientists access data inside Fabric?**

Using notebooks (Spark/Python) with direct access to **Lakehouse Delta tables** ‚Äî no data movement or import required.

---

### **Q2: How are ML models tracked inside Fabric?**

Fabric uses **MLflow** to automatically log:

* Metrics
* Parameters
* Model artifacts
* Run IDs
  This supports experiment comparison and version control.

---

### **Q3: What is the difference between training and scoring in Fabric?**

| Training           | Scoring                      |
| ------------------ | ---------------------------- |
| Builds the model   | Generates predictions        |
| Uses ML algorithms | Uses trained model           |
| Expensive compute  | Lightweight compute          |
| Done occasionally  | Batch or real-time frequency |

---

### **Q4: When should Azure ML be used instead of Fabric ML runtime?**

When high-performance GPU workloads, automated hyperparameter tuning, or enterprise MLOps workflows are required.

---

### **Q5: How do ML predictions appear in Power BI?**

Predictions are written to Lakehouse/Mart ‚Üí included in semantic model ‚Üí used as a field or KPI in Power BI.

---

### **Q6: Can Fabric support real-time ML?**

Yes ‚Äî via:

* Real-Time Analytics (KQL DB)
* REST scoring endpoints
* Power BI auto-refresh visuals

---

## ‚≠ê Quick Cheat Sheet

| Task                | Best Fabric Tool                                   |
| ------------------- | -------------------------------------------------- |
| Data preprocessing  | Notebook (PySpark)                                 |
| Model training      | Notebook + Synapse ML / sklearn                    |
| Model tracking      | MLflow                                             |
| Batch inference     | Notebook + Lakehouse writeback                     |
| Real-time inference | Azure ML endpoint or Fabric Real-time intelligence |
| Visualization       | Power BI                                           |
| Automation          | Pipelines & triggers                               |

---